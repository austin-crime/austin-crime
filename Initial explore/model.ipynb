{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434d46d5-d077-410d-be1b-d0d7561f75a0",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "This notebook containing all steps and decisions in the modeling phase of the pipeline.\n",
    "\n",
    "## The Required Imports\n",
    "\n",
    "Here we'll import all the modules required to run the code cells in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbe1263-648c-4421-91aa-5d05daf7823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from wrangle import wrangle_crime_data\n",
    "from prepare import split_data\n",
    "from evaluate import *\n",
    "from model import *\n",
    "\n",
    "# We'll use this random seed for all the machine learning models.\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6d0ba-ce2a-4e1b-a036-2dc636e3adfb",
   "metadata": {},
   "source": [
    "## Acquire, Prepare, and Split the Data\n",
    "\n",
    "Here we'll use the wrangle module to acquire and prepare the data. We'll then split the data into train, validate, and test datasets. The train dataset will be used to train the machine learning models. Validate and test will be used to determine how our models perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbf9665-8b4e-489a-87dc-83349fdd15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((195795, 392), (83913, 392), (69928, 392))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangle_crime_data()\n",
    "df = prep_data_for_modeling(df)\n",
    "\n",
    "train, validate, test = split_data(df)\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042fa0ac-8e3b-4c06-a82f-cca0c51aecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 195795 entries, 68896 to 178533\n",
      "Columns: 392 entries, council_district to month\n",
      "dtypes: bool(2), float64(1), int64(2), uint8(387)\n",
      "memory usage: 78.6 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3e164f-306c-46ad-abc7-ad2b842cb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, validate_scaled, test_scaled = train.copy(), validate.copy(), test.copy()\n",
    "\n",
    "columns = train_scaled.drop(columns = 'cleared').columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled[columns] = scaler.fit_transform(train[columns])\n",
    "validate_scaled[columns] = scaler.transform(validate[columns])\n",
    "test_scaled[columns] = scaler.transform(test[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3663d8e-5402-4b69-a002-d4f79a8b6661",
   "metadata": {},
   "source": [
    "## Establish a Baseline\n",
    "\n",
    "We will need to establish a baseline model which will serve as performance reference for our models. The baseline will simply use the simplest approach to predict clearance status (which will be simply predicting the most frequent value). With this reference point will be able to determine if our models at least perform better than the simplest model we could build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c99fdf8-c539-4e32-96b2-571bd5a16072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    195795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will establish a baseline model which will always predict the most frequent value in the target variable.\n",
    "\n",
    "baseline = establish_classification_baseline(train.cleared)\n",
    "baseline.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f085ee9-774e-405f-a939-81a0b8c96e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the roc auc score.\n",
    "roc_auc_score(train.cleared, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975b40e2-4edd-476e-8ed9-9129693b80c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7882887714190863"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score.\n",
    "accuracy_score(train.cleared, baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667aef3b-4975-4e74-aaf9-38636867d54f",
   "metadata": {},
   "source": [
    "We'll use two metrics to determine the performance of our models: roc auc score and accuracy. Accuracy will tell us how well the model predicts the clearance status of case for our dataset. However, due to the imbalance in our target variable we have to use another metric that will help determine in general how well the model predicts clearance status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3345622-cb92-447c-83f9-e2f15aeb24a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy  roc_auc\n",
       "baseline      0.79      0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = append_model_results('baseline', evaluate(train.cleared, baseline, True))\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d74cb7-3c77-4fda-95a5-47ca3e46e3f2",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Before we begin building machine learning models let's use RFE to determine the importance of the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252cdb95-fd66-4350-9737-6f0ce9452547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We'll use RFE to rank the importance of the features in the dataset. We'll use a decision tree classifier \n",
    "# # as the model to compare the features.\n",
    "\n",
    "# rfe = RFE(DecisionTreeClassifier(max_depth = 15), n_features_to_select = 2)\n",
    "# rfe.fit(train.drop(columns = 'cleared'), train.cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b61b7c4-8420-4c26-9f46-25ac49d8d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'Var': train.drop(columns = 'cleared').columns, 'Rank': rfe.ranking_}).sort_values(by = 'Rank').head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82dcf1-6dd6-48d8-bffb-ffec4630c359",
   "metadata": {},
   "source": [
    "## Initial Set of Models\n",
    "\n",
    "Now we will build a set of initial models to determine which ones have the best performance. We will try building models using various classification algorithms provided by sklearn. These models will be evaluated on the train dataset and the top 3 performing models will be evaluated on validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "365b8a2a-843c-474c-ba22-d4acd0ebb4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Bagging Classifier'] = Model(BaggingClassifier(random_state = random_seed), train = train, features = train.drop(columns = 'cleared').columns, target = 'cleared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a66e9fa-5049-40c8-bbde-67830b6a8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree model, 12.754553079605103 seconds\n",
      "Training Random Forest model, 36.80265283584595 seconds\n",
      "Training Ada Boost model, 251.447411775589 seconds\n",
      "Training Bagging Classifier model, 223.18360877037048 seconds\n",
      "Training Gradient Boosting model, 151.38878059387207 seconds\n",
      "Training SGD model, 10.347418069839478 seconds\n",
      "Training Naive Bayes model, 6.08551025390625 seconds\n"
     ]
    }
   ],
   "source": [
    "# All the machine learning model objects will be created using mostly default values with just a few exceptions \n",
    "# such as decision trees which will have a limited depth.\n",
    "\n",
    "algorithms = {\n",
    "    'Decision Tree' : DecisionTreeClassifier(max_depth = 15, random_state = random_seed),\n",
    "    'Random Forest' : RandomForestClassifier(max_depth = 15, random_state = random_seed),\n",
    "    'Ada Boost' : AdaBoostClassifier(random_state = random_seed),\n",
    "    'Bagging Classifier' : BaggingClassifier(random_state = random_seed),\n",
    "    'Gradient Boosting' : GradientBoostingClassifier(random_state = random_seed),\n",
    "    'SGD' : SGDClassifier(random_state = random_seed),\n",
    "    'Naive Bayes' : BernoulliNB()\n",
    "}\n",
    "\n",
    "models = {}\n",
    "\n",
    "for key, algorithm in algorithms.items():\n",
    "    print(f'Training {key} model, ', end = '')\n",
    "    \n",
    "    start = time()\n",
    "    models[key] = Model(\n",
    "        algorithm,\n",
    "        train = train,\n",
    "        features = train.drop(columns = 'cleared').columns,\n",
    "        target = 'cleared'\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a829d7b-95c7-4e13-a7df-165b1a4c5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree model, 6.028434991836548 seconds\n",
      "Evaluating Random Forest model, 8.273238897323608 seconds\n",
      "Evaluating Ada Boost model, 119.57484221458435 seconds\n",
      "Evaluating Bagging Classifier model, 33.803488969802856 seconds\n",
      "Evaluating Gradient Boosting model, 5.067091941833496 seconds\n",
      "Evaluating SGD model, 4.459805965423584 seconds\n",
      "Evaluating Naive Bayes model, 5.2549309730529785 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ada Boost</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy  roc_auc\n",
       "Bagging Classifier      0.96     0.93\n",
       "Naive Bayes             0.88     0.81\n",
       "Ada Boost               0.89     0.79\n",
       "Decision Tree           0.89     0.78\n",
       "Gradient Boosting       0.89     0.77\n",
       "SGD                     0.87     0.76\n",
       "Random Forest           0.86     0.68\n",
       "baseline                0.79     0.50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll evaluate the models.\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Evaluating {name} model, ', end = '')\n",
    "    \n",
    "    start = time()\n",
    "    eval_df = append_model_results(\n",
    "        name,\n",
    "        evaluate(train.cleared, model.make_predictions(train), True),\n",
    "        eval_df\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds')\n",
    "    \n",
    "eval_df.sort_values(by = 'roc_auc', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176fbc7-eb60-4b57-952a-01b53c9a2e60",
   "metadata": {},
   "source": [
    "For both metrics the four models with the best performance are the Bagging Classifier, KNN, Naive Bayes, and the Decision Tree. We'll now create some new models with different hyper-parameters for each of these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "883e249b-bdd0-4543-b125-8efde115f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes f:True a:0.0 model,  model, l, \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes f:False a:0.0 model, \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.279297590255737 secondslse a:1.0 model,  \r"
     ]
    }
   ],
   "source": [
    "# Create different versions of the top 4 models using various hyper-parameters.\n",
    "\n",
    "algorithms = {}\n",
    "\n",
    "# Create a variety of decision tree models with various hyper-parameter values for \n",
    "# max_depth, min_samples_leaf, and criterion.\n",
    "\n",
    "for max_depth in range(10, 26):\n",
    "    for min_samples_leaf in range(1, 6):\n",
    "        for criterion in ['gini', 'entropy']:\n",
    "            algorithms[\n",
    "                f'Decision Tree md:{max_depth} msl:{min_samples_leaf} c:{criterion}'\n",
    "            ] = DecisionTreeClassifier(\n",
    "                max_depth = max_depth,\n",
    "                min_samples_leaf = min_samples_leaf,\n",
    "                criterion = criterion,\n",
    "                random_state = random_seed\n",
    "            )\n",
    "            \n",
    "# Create a variety of adaboost classifier models with various hyper-parameters values for\n",
    "# n_estimators and base_estimator.\n",
    "\n",
    "for n_estimators in range(45, 56):\n",
    "    for name, base_estimator in {\n",
    "        'dt' : DecisionTreeClassifier(max_depth = 1, random_state = random_seed),\n",
    "        'rf' : RandomForestClassifier(max_depth = 1, random_state = random_seed)\n",
    "    }.items():\n",
    "        algorithms[\n",
    "            f'Ada Boost b:{name} n:{n_estimators}'\n",
    "        ] = AdaBoostClassifier(\n",
    "            base_estimator = base_estimator,\n",
    "            n_estimators = n_estimators,\n",
    "            random_state = random_seed\n",
    "        )\n",
    "        \n",
    "# Create a variety of bagging classifier models with various hyper-parameter values for \n",
    "# base_estimator, n_estimators, and max_features.\n",
    "\n",
    "for name, base_estimator in {\n",
    "    'dt' : DecisionTreeClassifier(max_depth = 1, random_state = random_seed),\n",
    "    'rf' : RandomForestClassifier(max_depth = 1, random_state = random_seed)\n",
    "}.items():\n",
    "    for n_estimators in range(8, 13):\n",
    "        for max_features in range(1, 3):\n",
    "            algorithms[\n",
    "                f'Bagging Classifier b:{name} n:{n_estimators} m:{max_features}'\n",
    "            ] = BaggingClassifier(\n",
    "                base_estimator = base_estimator,\n",
    "                n_estimators = n_estimators,\n",
    "                max_features = max_features,\n",
    "                random_state = random_seed\n",
    "            )\n",
    "            \n",
    "# Create a variety of naive bayes models with various hyper-parameter values for \n",
    "# fit_prior and alpha.\n",
    "\n",
    "for fit_prior in [True, False]:\n",
    "    for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "        algorithms[\n",
    "            f'Naive Bayes f:{fit_prior} a:{alpha}'\n",
    "        ] = BernoulliNB(fit_prior = fit_prior, alpha = alpha)\n",
    "\n",
    "models = {}\n",
    "\n",
    "for key, algorithm in algorithms.items():\n",
    "    print(f'Training {key} model, ', end = '\\r')\n",
    "    \n",
    "    start = time()\n",
    "    models[key] = Model(\n",
    "        algorithm,\n",
    "        train = train,\n",
    "        features = train.drop(columns = 'cleared').columns,\n",
    "        target = 'cleared'\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2919ac07-d632-4339-a064-69d9c5fea8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree md:10 msl:1 c:gini model, 3.9683518409729004 seconds\n",
      "Evaluating Decision Tree md:10 msl:1 c:entropy model, 3.9314963817596436 seconds\n",
      "Evaluating Decision Tree md:10 msl:2 c:gini model, 3.9557418823242188 seconds\n",
      "Evaluating Decision Tree md:10 msl:2 c:entropy model, 3.9401259422302246 seconds\n",
      "Evaluating Decision Tree md:10 msl:3 c:gini model, 3.954310894012451 seconds\n",
      "Evaluating Decision Tree md:10 msl:3 c:entropy model, 3.924078941345215 seconds\n",
      "Evaluating Decision Tree md:10 msl:4 c:gini model, 3.9283089637756348 seconds\n",
      "Evaluating Decision Tree md:10 msl:4 c:entropy model, 3.9114458560943604 seconds\n",
      "Evaluating Decision Tree md:10 msl:5 c:gini model, 3.950137138366699 seconds\n",
      "Evaluating Decision Tree md:10 msl:5 c:entropy model, 3.927891969680786 seconds\n",
      "Evaluating Decision Tree md:11 msl:1 c:gini model, 3.955997943878174 seconds\n",
      "Evaluating Decision Tree md:11 msl:1 c:entropy model, 3.935448169708252 seconds\n",
      "Evaluating Decision Tree md:11 msl:2 c:gini model, 3.910248041152954 seconds\n",
      "Evaluating Decision Tree md:11 msl:2 c:entropy model, 3.9431917667388916 seconds\n",
      "Evaluating Decision Tree md:11 msl:3 c:gini model, 3.939908027648926 seconds\n",
      "Evaluating Decision Tree md:11 msl:3 c:entropy model, 3.9208099842071533 seconds\n",
      "Evaluating Decision Tree md:11 msl:4 c:gini model, 3.9226760864257812 seconds\n",
      "Evaluating Decision Tree md:11 msl:4 c:entropy model, 3.9283578395843506 seconds\n",
      "Evaluating Decision Tree md:11 msl:5 c:gini model, 3.8809409141540527 seconds\n",
      "Evaluating Decision Tree md:11 msl:5 c:entropy model, 3.8304688930511475 seconds\n",
      "Evaluating Decision Tree md:12 msl:1 c:gini model, 3.8168022632598877 seconds\n",
      "Evaluating Decision Tree md:12 msl:1 c:entropy model, 3.8246030807495117 seconds\n",
      "Evaluating Decision Tree md:12 msl:2 c:gini model, 3.8526880741119385 seconds\n",
      "Evaluating Decision Tree md:12 msl:2 c:entropy model, 3.8773369789123535 seconds\n",
      "Evaluating Decision Tree md:12 msl:3 c:gini model, 3.823837995529175 seconds\n",
      "Evaluating Decision Tree md:12 msl:3 c:entropy model, 3.8132741451263428 seconds\n",
      "Evaluating Decision Tree md:12 msl:4 c:gini model, 3.8223109245300293 seconds\n",
      "Evaluating Decision Tree md:12 msl:4 c:entropy model, 3.813051700592041 seconds\n",
      "Evaluating Decision Tree md:12 msl:5 c:gini model, 3.851233959197998 seconds\n",
      "Evaluating Decision Tree md:12 msl:5 c:entropy model, 3.8254919052124023 seconds\n",
      "Evaluating Decision Tree md:13 msl:1 c:gini model, 3.862807035446167 seconds\n",
      "Evaluating Decision Tree md:13 msl:1 c:entropy model, 3.840730905532837 seconds\n",
      "Evaluating Decision Tree md:13 msl:2 c:gini model, 3.833111047744751 seconds\n",
      "Evaluating Decision Tree md:13 msl:2 c:entropy model, 3.8416638374328613 seconds\n",
      "Evaluating Decision Tree md:13 msl:3 c:gini model, 3.82425594329834 seconds\n",
      "Evaluating Decision Tree md:13 msl:3 c:entropy model, 3.8127248287200928 seconds\n",
      "Evaluating Decision Tree md:13 msl:4 c:gini model, 3.8262650966644287 seconds\n",
      "Evaluating Decision Tree md:13 msl:4 c:entropy model, 3.827455997467041 seconds\n",
      "Evaluating Decision Tree md:13 msl:5 c:gini model, 3.8496017456054688 seconds\n",
      "Evaluating Decision Tree md:13 msl:5 c:entropy model, 3.819099187850952 seconds\n",
      "Evaluating Decision Tree md:14 msl:1 c:gini model, 3.8280491828918457 seconds\n",
      "Evaluating Decision Tree md:14 msl:1 c:entropy model, 3.8557322025299072 seconds\n",
      "Evaluating Decision Tree md:14 msl:2 c:gini model, 3.84151291847229 seconds\n",
      "Evaluating Decision Tree md:14 msl:2 c:entropy model, 3.8670589923858643 seconds\n",
      "Evaluating Decision Tree md:14 msl:3 c:gini model, 3.815401315689087 seconds\n",
      "Evaluating Decision Tree md:14 msl:3 c:entropy model, 3.818152904510498 seconds\n",
      "Evaluating Decision Tree md:14 msl:4 c:gini model, 3.829470157623291 seconds\n",
      "Evaluating Decision Tree md:14 msl:4 c:entropy model, 3.8369908332824707 seconds\n",
      "Evaluating Decision Tree md:14 msl:5 c:gini model, 3.8737690448760986 seconds\n",
      "Evaluating Decision Tree md:14 msl:5 c:entropy model, 3.869987726211548 seconds\n",
      "Evaluating Decision Tree md:15 msl:1 c:gini model, 3.8593380451202393 seconds\n",
      "Evaluating Decision Tree md:15 msl:1 c:entropy model, 3.818833351135254 seconds\n",
      "Evaluating Decision Tree md:15 msl:2 c:gini model, 3.822478771209717 seconds\n",
      "Evaluating Decision Tree md:15 msl:2 c:entropy model, 3.858654022216797 seconds\n",
      "Evaluating Decision Tree md:15 msl:3 c:gini model, 3.858052968978882 seconds\n",
      "Evaluating Decision Tree md:15 msl:3 c:entropy model, 3.8338589668273926 seconds\n",
      "Evaluating Decision Tree md:15 msl:4 c:gini model, 3.8264787197113037 seconds\n",
      "Evaluating Decision Tree md:15 msl:4 c:entropy model, 3.8199832439422607 seconds\n",
      "Evaluating Decision Tree md:15 msl:5 c:gini model, 3.819279670715332 seconds\n",
      "Evaluating Decision Tree md:15 msl:5 c:entropy model, 3.831364870071411 seconds\n",
      "Evaluating Decision Tree md:16 msl:1 c:gini model, 3.8229498863220215 seconds\n",
      "Evaluating Decision Tree md:16 msl:1 c:entropy model, 3.8662819862365723 seconds\n",
      "Evaluating Decision Tree md:16 msl:2 c:gini model, 3.8234810829162598 seconds\n",
      "Evaluating Decision Tree md:16 msl:2 c:entropy model, 3.8186161518096924 seconds\n",
      "Evaluating Decision Tree md:16 msl:3 c:gini model, 3.8214120864868164 seconds\n",
      "Evaluating Decision Tree md:16 msl:3 c:entropy model, 3.8500590324401855 seconds\n",
      "Evaluating Decision Tree md:16 msl:4 c:gini model, 3.8328211307525635 seconds\n",
      "Evaluating Decision Tree md:16 msl:4 c:entropy model, 3.8427209854125977 seconds\n",
      "Evaluating Decision Tree md:16 msl:5 c:gini model, 3.8384172916412354 seconds\n",
      "Evaluating Decision Tree md:16 msl:5 c:entropy model, 3.845567226409912 seconds\n",
      "Evaluating Decision Tree md:17 msl:1 c:gini model, 3.8454630374908447 seconds\n",
      "Evaluating Decision Tree md:17 msl:1 c:entropy model, 3.818650722503662 seconds\n",
      "Evaluating Decision Tree md:17 msl:2 c:gini model, 3.8190131187438965 seconds\n",
      "Evaluating Decision Tree md:17 msl:2 c:entropy model, 3.831743001937866 seconds\n",
      "Evaluating Decision Tree md:17 msl:3 c:gini model, 3.821559190750122 seconds\n",
      "Evaluating Decision Tree md:17 msl:3 c:entropy model, 3.8289480209350586 seconds\n",
      "Evaluating Decision Tree md:17 msl:4 c:gini model, 3.8299450874328613 seconds\n",
      "Evaluating Decision Tree md:17 msl:4 c:entropy model, 3.824803113937378 seconds\n",
      "Evaluating Decision Tree md:17 msl:5 c:gini model, 3.844454050064087 seconds\n",
      "Evaluating Decision Tree md:17 msl:5 c:entropy model, 3.8327748775482178 seconds\n",
      "Evaluating Decision Tree md:18 msl:1 c:gini model, 3.834831714630127 seconds\n",
      "Evaluating Decision Tree md:18 msl:1 c:entropy model, 3.8459689617156982 seconds\n",
      "Evaluating Decision Tree md:18 msl:2 c:gini model, 3.8397488594055176 seconds\n",
      "Evaluating Decision Tree md:18 msl:2 c:entropy model, 3.8239920139312744 seconds\n",
      "Evaluating Decision Tree md:18 msl:3 c:gini model, 3.830014944076538 seconds\n",
      "Evaluating Decision Tree md:18 msl:3 c:entropy model, 3.8336730003356934 seconds\n",
      "Evaluating Decision Tree md:18 msl:4 c:gini model, 3.827402114868164 seconds\n",
      "Evaluating Decision Tree md:18 msl:4 c:entropy model, 3.828080892562866 seconds\n",
      "Evaluating Decision Tree md:18 msl:5 c:gini model, 3.831555128097534 seconds\n",
      "Evaluating Decision Tree md:18 msl:5 c:entropy model, 3.829871892929077 seconds\n",
      "Evaluating Decision Tree md:19 msl:1 c:gini model, 3.8347980976104736 seconds\n",
      "Evaluating Decision Tree md:19 msl:1 c:entropy model, 3.8540780544281006 seconds\n",
      "Evaluating Decision Tree md:19 msl:2 c:gini model, 3.8607640266418457 seconds\n",
      "Evaluating Decision Tree md:19 msl:2 c:entropy model, 3.8757450580596924 seconds\n",
      "Evaluating Decision Tree md:19 msl:3 c:gini model, 3.855755090713501 seconds\n",
      "Evaluating Decision Tree md:19 msl:3 c:entropy model, 3.8376190662384033 seconds\n",
      "Evaluating Decision Tree md:19 msl:4 c:gini model, 3.8427469730377197 seconds\n",
      "Evaluating Decision Tree md:19 msl:4 c:entropy model, 3.8585150241851807 seconds\n",
      "Evaluating Decision Tree md:19 msl:5 c:gini model, 3.826519012451172 seconds\n",
      "Evaluating Decision Tree md:19 msl:5 c:entropy model, 3.857318162918091 seconds\n",
      "Evaluating Decision Tree md:20 msl:1 c:gini model, 3.8704640865325928 seconds\n",
      "Evaluating Decision Tree md:20 msl:1 c:entropy model, 3.846930742263794 seconds\n",
      "Evaluating Decision Tree md:20 msl:2 c:gini model, 3.845144033432007 seconds\n",
      "Evaluating Decision Tree md:20 msl:2 c:entropy model, 3.851693868637085 seconds\n",
      "Evaluating Decision Tree md:20 msl:3 c:gini model, 3.8279078006744385 seconds\n",
      "Evaluating Decision Tree md:20 msl:3 c:entropy model, 3.8357291221618652 seconds\n",
      "Evaluating Decision Tree md:20 msl:4 c:gini model, 3.828144073486328 seconds\n",
      "Evaluating Decision Tree md:20 msl:4 c:entropy model, 3.846224069595337 seconds\n",
      "Evaluating Decision Tree md:20 msl:5 c:gini model, 3.823007106781006 seconds\n",
      "Evaluating Decision Tree md:20 msl:5 c:entropy model, 3.8543832302093506 seconds\n",
      "Evaluating Decision Tree md:21 msl:1 c:gini model, 3.8288519382476807 seconds\n",
      "Evaluating Decision Tree md:21 msl:1 c:entropy model, 3.835495948791504 seconds\n",
      "Evaluating Decision Tree md:21 msl:2 c:gini model, 3.835278272628784 seconds\n",
      "Evaluating Decision Tree md:21 msl:2 c:entropy model, 3.838939905166626 seconds\n",
      "Evaluating Decision Tree md:21 msl:3 c:gini model, 3.8596601486206055 seconds\n",
      "Evaluating Decision Tree md:21 msl:3 c:entropy model, 3.8485751152038574 seconds\n",
      "Evaluating Decision Tree md:21 msl:4 c:gini model, 3.838273048400879 seconds\n",
      "Evaluating Decision Tree md:21 msl:4 c:entropy model, 3.8471388816833496 seconds\n",
      "Evaluating Decision Tree md:21 msl:5 c:gini model, 3.848037004470825 seconds\n",
      "Evaluating Decision Tree md:21 msl:5 c:entropy model, 3.8405890464782715 seconds\n",
      "Evaluating Decision Tree md:22 msl:1 c:gini model, 3.8510541915893555 seconds\n",
      "Evaluating Decision Tree md:22 msl:1 c:entropy model, 3.8408169746398926 seconds\n",
      "Evaluating Decision Tree md:22 msl:2 c:gini model, 3.8381588459014893 seconds\n",
      "Evaluating Decision Tree md:22 msl:2 c:entropy model, 3.8551549911499023 seconds\n",
      "Evaluating Decision Tree md:22 msl:3 c:gini model, 3.871858835220337 seconds\n",
      "Evaluating Decision Tree md:22 msl:3 c:entropy model, 3.8487260341644287 seconds\n",
      "Evaluating Decision Tree md:22 msl:4 c:gini model, 3.9090981483459473 seconds\n",
      "Evaluating Decision Tree md:22 msl:4 c:entropy model, 3.845180034637451 seconds\n",
      "Evaluating Decision Tree md:22 msl:5 c:gini model, 3.8740999698638916 seconds\n",
      "Evaluating Decision Tree md:22 msl:5 c:entropy model, 3.8317391872406006 seconds\n",
      "Evaluating Decision Tree md:23 msl:1 c:gini model, 3.8532402515411377 seconds\n",
      "Evaluating Decision Tree md:23 msl:1 c:entropy model, 3.8384251594543457 seconds\n",
      "Evaluating Decision Tree md:23 msl:2 c:gini model, 3.8583219051361084 seconds\n",
      "Evaluating Decision Tree md:23 msl:2 c:entropy model, 3.8407602310180664 seconds\n",
      "Evaluating Decision Tree md:23 msl:3 c:gini model, 3.852755069732666 seconds\n",
      "Evaluating Decision Tree md:23 msl:3 c:entropy model, 3.8411688804626465 seconds\n",
      "Evaluating Decision Tree md:23 msl:4 c:gini model, 3.845648765563965 seconds\n",
      "Evaluating Decision Tree md:23 msl:4 c:entropy model, 3.8602969646453857 seconds\n",
      "Evaluating Decision Tree md:23 msl:5 c:gini model, 3.834512948989868 seconds\n",
      "Evaluating Decision Tree md:23 msl:5 c:entropy model, 3.8483028411865234 seconds\n",
      "Evaluating Decision Tree md:24 msl:1 c:gini model, 3.827590227127075 seconds\n",
      "Evaluating Decision Tree md:24 msl:1 c:entropy model, 3.8519179821014404 seconds\n",
      "Evaluating Decision Tree md:24 msl:2 c:gini model, 3.8327980041503906 seconds\n",
      "Evaluating Decision Tree md:24 msl:2 c:entropy model, 3.835676908493042 seconds\n",
      "Evaluating Decision Tree md:24 msl:3 c:gini model, 3.844774007797241 seconds\n",
      "Evaluating Decision Tree md:24 msl:3 c:entropy model, 3.8333680629730225 seconds\n",
      "Evaluating Decision Tree md:24 msl:4 c:gini model, 3.8837971687316895 seconds\n",
      "Evaluating Decision Tree md:24 msl:4 c:entropy model, 3.8450818061828613 seconds\n",
      "Evaluating Decision Tree md:24 msl:5 c:gini model, 3.8441998958587646 seconds\n",
      "Evaluating Decision Tree md:24 msl:5 c:entropy model, 3.8372321128845215 seconds\n",
      "Evaluating Decision Tree md:25 msl:1 c:gini model, 3.890721082687378 seconds\n",
      "Evaluating Decision Tree md:25 msl:1 c:entropy model, 3.921205997467041 seconds\n",
      "Evaluating Decision Tree md:25 msl:2 c:gini model, 3.8323981761932373 seconds\n",
      "Evaluating Decision Tree md:25 msl:2 c:entropy model, 3.9974358081817627 seconds\n",
      "Evaluating Decision Tree md:25 msl:3 c:gini model, 4.040138006210327 seconds\n",
      "Evaluating Decision Tree md:25 msl:3 c:entropy model, 4.00066876411438 seconds\n",
      "Evaluating Decision Tree md:25 msl:4 c:gini model, 3.8662970066070557 seconds\n",
      "Evaluating Decision Tree md:25 msl:4 c:entropy model, 3.8735177516937256 seconds\n",
      "Evaluating Decision Tree md:25 msl:5 c:gini model, 3.8467180728912354 seconds\n",
      "Evaluating Decision Tree md:25 msl:5 c:entropy model, 3.8371450901031494 seconds\n",
      "Evaluating Ada Boost b:dt n:45 model, 100.53355407714844 seconds\n",
      "Evaluating Ada Boost b:rf n:45 model, 134.70606422424316 seconds\n",
      "Evaluating Ada Boost b:dt n:46 model, 102.84644293785095 seconds\n",
      "Evaluating Ada Boost b:rf n:46 model, 137.65329885482788 seconds\n",
      "Evaluating Ada Boost b:dt n:47 model, 104.79373502731323 seconds\n",
      "Evaluating Ada Boost b:rf n:47 model, 140.45958995819092 seconds\n",
      "Evaluating Ada Boost b:dt n:48 model, 107.21109890937805 seconds\n",
      "Evaluating Ada Boost b:rf n:48 model, 143.47883582115173 seconds\n",
      "Evaluating Ada Boost b:dt n:49 model, 109.22989320755005 seconds\n",
      "Evaluating Ada Boost b:rf n:49 model, 146.25859808921814 seconds\n",
      "Evaluating Ada Boost b:dt n:50 model, 111.1946370601654 seconds\n",
      "Evaluating Ada Boost b:rf n:50 model, 149.27336716651917 seconds\n",
      "Evaluating Ada Boost b:dt n:51 model, 113.57161402702332 seconds\n",
      "Evaluating Ada Boost b:rf n:51 model, 152.1628098487854 seconds\n",
      "Evaluating Ada Boost b:dt n:52 model, 115.6192102432251 seconds\n",
      "Evaluating Ada Boost b:rf n:52 model, 155.16099786758423 seconds\n",
      "Evaluating Ada Boost b:dt n:53 model, 117.56055688858032 seconds\n",
      "Evaluating Ada Boost b:rf n:53 model, 158.30212306976318 seconds\n",
      "Evaluating Ada Boost b:dt n:54 model, 119.75718307495117 seconds\n",
      "Evaluating Ada Boost b:rf n:54 model, 160.58303689956665 seconds\n",
      "Evaluating Ada Boost b:dt n:55 model, 121.97309994697571 seconds\n",
      "Evaluating Ada Boost b:rf n:55 model, 163.58943629264832 seconds\n",
      "Evaluating Bagging Classifier b:dt n:8 m:1 model, 1.7749919891357422 seconds\n",
      "Evaluating Bagging Classifier b:dt n:8 m:2 model, 1.8388569355010986 seconds\n",
      "Evaluating Bagging Classifier b:dt n:9 m:1 model, 1.8401710987091064 seconds\n",
      "Evaluating Bagging Classifier b:dt n:9 m:2 model, 1.8381080627441406 seconds\n",
      "Evaluating Bagging Classifier b:dt n:10 m:1 model, 1.7905378341674805 seconds\n",
      "Evaluating Bagging Classifier b:dt n:10 m:2 model, 1.8767375946044922 seconds\n",
      "Evaluating Bagging Classifier b:dt n:11 m:1 model, 1.859532117843628 seconds\n",
      "Evaluating Bagging Classifier b:dt n:11 m:2 model, 1.8923656940460205 seconds\n",
      "Evaluating Bagging Classifier b:dt n:12 m:1 model, 1.8566291332244873 seconds\n",
      "Evaluating Bagging Classifier b:dt n:12 m:2 model, 1.9001939296722412 seconds\n",
      "Evaluating Bagging Classifier b:rf n:8 m:1 model, 7.38343620300293 seconds\n",
      "Evaluating Bagging Classifier b:rf n:8 m:2 model, 7.833385944366455 seconds\n",
      "Evaluating Bagging Classifier b:rf n:9 m:1 model, 8.532498121261597 seconds\n",
      "Evaluating Bagging Classifier b:rf n:9 m:2 model, 8.624732971191406 seconds\n",
      "Evaluating Bagging Classifier b:rf n:10 m:1 model, 9.201272010803223 seconds\n",
      "Evaluating Bagging Classifier b:rf n:10 m:2 model, 9.509347915649414 seconds\n",
      "Evaluating Bagging Classifier b:rf n:11 m:1 model, 10.09030795097351 seconds\n",
      "Evaluating Bagging Classifier b:rf n:11 m:2 model, 10.263783931732178 seconds\n",
      "Evaluating Bagging Classifier b:rf n:12 m:1 model, 10.806691884994507 seconds\n",
      "Evaluating Bagging Classifier b:rf n:12 m:2 model, 11.024502038955688 seconds\n",
      "Evaluating Naive Bayes f:True a:0.0 model, 5.404538869857788 seconds\n",
      "Evaluating Naive Bayes f:True a:0.25 model, 5.3448731899261475 seconds\n",
      "Evaluating Naive Bayes f:True a:0.5 model, 5.296871900558472 seconds\n",
      "Evaluating Naive Bayes f:True a:0.75 model, 5.216429710388184 seconds\n",
      "Evaluating Naive Bayes f:True a:1.0 model, 5.187963962554932 seconds\n",
      "Evaluating Naive Bayes f:False a:0.0 model, 5.155974864959717 seconds\n",
      "Evaluating Naive Bayes f:False a:0.25 model, 5.184946060180664 seconds\n",
      "Evaluating Naive Bayes f:False a:0.5 model, 5.161355972290039 seconds\n",
      "Evaluating Naive Bayes f:False a:0.75 model, 5.155659914016724 seconds\n",
      "Evaluating Naive Bayes f:False a:1.0 model, 5.146275043487549 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:1.0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.75</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.5</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.25</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier b:dt n:9 m:2</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier b:dt n:9 m:1</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier b:dt n:8 m:2</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier b:dt n:8 m:1</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 accuracy  roc_auc\n",
       "Bagging Classifier                   0.96     0.93\n",
       "Naive Bayes f:False a:1.0            0.84     0.84\n",
       "Naive Bayes f:False a:0.75           0.84     0.84\n",
       "Naive Bayes f:False a:0.5            0.84     0.84\n",
       "Naive Bayes f:False a:0.25           0.84     0.84\n",
       "...                                   ...      ...\n",
       "Bagging Classifier b:dt n:9 m:2      0.79     0.50\n",
       "Bagging Classifier b:dt n:9 m:1      0.79     0.50\n",
       "Bagging Classifier b:dt n:8 m:2      0.79     0.50\n",
       "Bagging Classifier b:dt n:8 m:1      0.79     0.50\n",
       "baseline                             0.79     0.50\n",
       "\n",
       "[220 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll evaluate the models.\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Evaluating {name} model, ', end = '')\n",
    "    \n",
    "    start = time()\n",
    "    eval_df = append_model_results(\n",
    "        name,\n",
    "        evaluate(train.cleared, model.make_predictions(train), True),\n",
    "        eval_df\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds')\n",
    "    \n",
    "eval_df.sort_values(by = 'roc_auc', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f3a8d4-f4f4-4e0d-944d-0974838d65ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:1.0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.75</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.5</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.25</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:25 msl:1 c:gini</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:24 msl:1 c:gini</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:25 msl:1 c:entropy</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:25 msl:2 c:gini</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     accuracy  roc_auc\n",
       "Bagging Classifier                       0.96     0.93\n",
       "Naive Bayes f:False a:1.0                0.84     0.84\n",
       "Naive Bayes f:False a:0.75               0.84     0.84\n",
       "Naive Bayes f:False a:0.5                0.84     0.84\n",
       "Naive Bayes f:False a:0.25               0.84     0.84\n",
       "Naive Bayes f:False a:0.0                0.84     0.84\n",
       "Decision Tree md:25 msl:1 c:gini         0.91     0.83\n",
       "Decision Tree md:24 msl:1 c:gini         0.91     0.82\n",
       "Decision Tree md:25 msl:1 c:entropy      0.91     0.82\n",
       "Decision Tree md:25 msl:2 c:gini         0.91     0.82"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.sort_values(by = 'roc_auc', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e362749-cfcf-4b1e-8370-0a3ce28db4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Bagging Classifier model, 12.927634000778198 seconds\n",
      "Evaluating Naive Bayes f:False a:1.0 model, 2.36434268951416 seconds\n",
      "Evaluating Naive Bayes f:False a:0.75 model, 2.3913583755493164 seconds\n",
      "Evaluating Naive Bayes f:False a:0.5 model, 2.3695759773254395 seconds\n",
      "Evaluating Naive Bayes f:False a:0.25 model, 2.3552420139312744 seconds\n",
      "Evaluating Naive Bayes f:False a:0.0 model, 2.3753459453582764 seconds\n",
      "Evaluating Decision Tree md:25 msl:1 c:gini model, 1.6748161315917969 seconds\n",
      "Evaluating Decision Tree md:24 msl:1 c:gini model, 1.6745269298553467 seconds\n",
      "Evaluating Decision Tree md:25 msl:1 c:entropy model, 1.6799218654632568 seconds\n",
      "Evaluating Decision Tree md:25 msl:2 c:gini model, 1.6673331260681152 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:1.0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.75</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.5</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.25</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes f:False a:0.0</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:25 msl:1 c:gini</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:24 msl:1 c:gini</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:25 msl:1 c:entropy</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree md:25 msl:2 c:gini</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     accuracy  roc_auc\n",
       "Naive Bayes f:False a:1.0                0.84     0.84\n",
       "Naive Bayes f:False a:0.75               0.84     0.84\n",
       "Naive Bayes f:False a:0.5                0.84     0.84\n",
       "Naive Bayes f:False a:0.25               0.84     0.84\n",
       "Naive Bayes f:False a:0.0                0.84     0.84\n",
       "Bagging Classifier                       0.89     0.82\n",
       "Decision Tree md:25 msl:1 c:gini         0.89     0.79\n",
       "Decision Tree md:24 msl:1 c:gini         0.89     0.78\n",
       "Decision Tree md:25 msl:1 c:entropy      0.88     0.78\n",
       "Decision Tree md:25 msl:2 c:gini         0.89     0.78"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll evaluate the top 3 performing models on validate.\n",
    "algorithms = [\n",
    "    'Bagging Classifier',\n",
    "    'Naive Bayes f:False a:1.0',\n",
    "    'Naive Bayes f:False a:0.75',\n",
    "    'Naive Bayes f:False a:0.5',\n",
    "    'Naive Bayes f:False a:0.25',\n",
    "    'Naive Bayes f:False a:0.0',\n",
    "    'Decision Tree md:25 msl:1 c:gini',\n",
    "    'Decision Tree md:24 msl:1 c:gini',\n",
    "    'Decision Tree md:25 msl:1 c:entropy',\n",
    "    'Decision Tree md:25 msl:2 c:gini'\n",
    "]\n",
    "\n",
    "eval_df = None\n",
    "\n",
    "for model in algorithms:\n",
    "    print(f'Evaluating {model} model, ', end = '')\n",
    "    \n",
    "    start = time()\n",
    "    eval_df = append_model_results(\n",
    "        model,\n",
    "        evaluate(validate.cleared, models[model].make_predictions(validate), True),\n",
    "        eval_df\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds')\n",
    "    \n",
    "eval_df.sort_values(by = 'roc_auc', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89056e3-ede5-41d8-9bf7-2cc2c67282e3",
   "metadata": {},
   "source": [
    "The Naive Bayes model has the same performance on validate as it does on train so we'll evaluate this one on test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8526151-b8c7-4da3-a6ef-827111ab04f3",
   "metadata": {},
   "source": [
    "## Trying Different Numbers of Features\n",
    "\n",
    "Now we're going to build some models that use a variety of different numbers of features to see how this affects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29926ee0-5fef-4f66-81fd-e70e4f30573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 38  40  49  62  63  64  68  77 102 103 120 154 207 272 277 288 322 324] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes fit_prior = False model k = 100, \r"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "# Select features using SelectKBest for the k = 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100.\n",
    "for k in range(10, 101, 10):\n",
    "    f_selector = SelectKBest(f_classif, k = k)\n",
    "    f_selector.fit(train_scaled.drop(columns = 'cleared'), train_scaled.cleared)\n",
    "    \n",
    "    # boolean mask of whether the column was selected or not. \n",
    "    feature_mask = f_selector.get_support()\n",
    "\n",
    "    # get list of top K features. \n",
    "    f_feature = train_scaled.drop(columns = 'cleared').iloc[:,feature_mask].columns.tolist()\n",
    "    \n",
    "    features.append(f_feature)\n",
    "    \n",
    "models = {}\n",
    "\n",
    "for feature_set in features:\n",
    "    print(f'Training Bagging Classifier model k = {len(feature_set)}, ', end = '\\r')\n",
    "    \n",
    "    models[f'Bagging Classifier k:{len(feature_set)}'] = Model(\n",
    "        BaggingClassifier(random_state = random_seed),\n",
    "        train = train_scaled,\n",
    "        features = feature_set,\n",
    "        target = 'cleared'\n",
    "    )\n",
    "    \n",
    "for feature_set in features:\n",
    "    print(f'Training Decision Tree model k = {len(feature_set)}, ', end = '\\r')\n",
    "    \n",
    "    models[f'Decision Tree k:{len(feature_set)}'] = Model(\n",
    "        DecisionTreeClassifier(max_depth = 25, random_state = random_seed),\n",
    "        train = train_scaled,\n",
    "        features = feature_set,\n",
    "        target = 'cleared'\n",
    "    )\n",
    "    \n",
    "for feature_set in features:\n",
    "    print(f'Training Naive Bayes model k = {len(feature_set)}, ', end = '\\r')\n",
    "    \n",
    "    models[f'Naive Bayes k:{len(feature_set)}'] = Model(\n",
    "        BernoulliNB(),\n",
    "        train = train_scaled,\n",
    "        features = feature_set,\n",
    "        target = 'cleared'\n",
    "    )\n",
    "    \n",
    "for feature_set in features:\n",
    "    print(f'Training Naive Bayes fit_prior = False model k = {len(feature_set)}, ', end = '\\r')\n",
    "    \n",
    "    models[f'Naive Bayes fit_prior:False k:{len(feature_set)}'] = Model(\n",
    "        BernoulliNB(fit_prior = False),\n",
    "        train = train_scaled,\n",
    "        features = feature_set,\n",
    "        target = 'cleared'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57faf5df-342a-422a-9bc3-952f7da82980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Bagging Classifier k:10 model, 0.33852314949035645 seconds\n",
      "Evaluating Bagging Classifier k:20 model, 0.47135400772094727 seconds\n",
      "Evaluating Bagging Classifier k:30 model, 0.965256929397583 seconds\n",
      "Evaluating Bagging Classifier k:40 model, 1.025602102279663 seconds\n",
      "Evaluating Bagging Classifier k:50 model, 1.1582262516021729 seconds\n",
      "Evaluating Bagging Classifier k:60 model, 1.3304362297058105 seconds\n",
      "Evaluating Bagging Classifier k:70 model, 1.6224617958068848 seconds\n",
      "Evaluating Bagging Classifier k:80 model, 1.819997787475586 seconds\n",
      "Evaluating Bagging Classifier k:90 model, 3.3077950477600098 seconds\n",
      "Evaluating Bagging Classifier k:100 model, 4.160368919372559 seconds\n",
      "Evaluating Decision Tree k:10 model, 0.07445311546325684 seconds\n",
      "Evaluating Decision Tree k:20 model, 0.08928298950195312 seconds\n",
      "Evaluating Decision Tree k:30 model, 0.11636710166931152 seconds\n",
      "Evaluating Decision Tree k:40 model, 0.1308748722076416 seconds\n",
      "Evaluating Decision Tree k:50 model, 0.1454448699951172 seconds\n",
      "Evaluating Decision Tree k:60 model, 0.16288375854492188 seconds\n",
      "Evaluating Decision Tree k:70 model, 0.17576193809509277 seconds\n",
      "Evaluating Decision Tree k:80 model, 0.1924448013305664 seconds\n",
      "Evaluating Decision Tree k:90 model, 0.2114708423614502 seconds\n",
      "Evaluating Decision Tree k:100 model, 0.26291394233703613 seconds\n",
      "Evaluating Naive Bayes k:10 model, 0.11240696907043457 seconds\n",
      "Evaluating Naive Bayes k:20 model, 0.15718817710876465 seconds\n",
      "Evaluating Naive Bayes k:30 model, 0.17856884002685547 seconds\n",
      "Evaluating Naive Bayes k:40 model, 0.2413332462310791 seconds\n",
      "Evaluating Naive Bayes k:50 model, 0.29273390769958496 seconds\n",
      "Evaluating Naive Bayes k:60 model, 0.29925084114074707 seconds\n",
      "Evaluating Naive Bayes k:70 model, 0.35588979721069336 seconds\n",
      "Evaluating Naive Bayes k:80 model, 0.3973691463470459 seconds\n",
      "Evaluating Naive Bayes k:90 model, 0.42253971099853516 seconds\n",
      "Evaluating Naive Bayes k:100 model, 0.4504249095916748 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:10 model, 0.08115696907043457 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:20 model, 0.1332700252532959 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:30 model, 0.18118619918823242 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:40 model, 0.23171305656433105 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:50 model, 0.3143329620361328 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:60 model, 0.3371000289916992 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:70 model, 0.40257692337036133 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:80 model, 0.4987671375274658 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:90 model, 0.535651683807373 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:100 model, 0.5894410610198975 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:100</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:90</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:80</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:70</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:60</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:100</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:50</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:90</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:80</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:90</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:80</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:70</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:60</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:100</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:60</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:70</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:40</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:50</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:50</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:30</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:40</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:30</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:40</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:30</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:20</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:100</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:80</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:90</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:20</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:10</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:70</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:60</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:50</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:20</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:40</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:30</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:20</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes k:10</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:10</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:10</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   accuracy  roc_auc\n",
       "Bagging Classifier k:100               0.95     0.91\n",
       "Bagging Classifier k:90                0.94     0.90\n",
       "Bagging Classifier k:80                0.93     0.88\n",
       "Bagging Classifier k:70                0.91     0.85\n",
       "Bagging Classifier k:60                0.91     0.84\n",
       "Decision Tree k:100                    0.91     0.83\n",
       "Bagging Classifier k:50                0.91     0.83\n",
       "Decision Tree k:90                     0.91     0.83\n",
       "Decision Tree k:80                     0.91     0.82\n",
       "Naive Bayes fit_prior:False k:90       0.84     0.82\n",
       "Naive Bayes fit_prior:False k:80       0.83     0.82\n",
       "Naive Bayes fit_prior:False k:70       0.83     0.82\n",
       "Naive Bayes fit_prior:False k:60       0.86     0.82\n",
       "Naive Bayes fit_prior:False k:100      0.84     0.82\n",
       "Decision Tree k:60                     0.90     0.81\n",
       "Decision Tree k:70                     0.90     0.81\n",
       "Bagging Classifier k:40                0.90     0.81\n",
       "Naive Bayes fit_prior:False k:50       0.85     0.81\n",
       "Decision Tree k:50                     0.90     0.81\n",
       "Bagging Classifier k:30                0.90     0.80\n",
       "Naive Bayes fit_prior:False k:40       0.86     0.80\n",
       "Naive Bayes fit_prior:False k:30       0.85     0.80\n",
       "Decision Tree k:40                     0.89     0.79\n",
       "Decision Tree k:30                     0.89     0.78\n",
       "Naive Bayes fit_prior:False k:20       0.85     0.78\n",
       "Naive Bayes k:100                      0.87     0.77\n",
       "Naive Bayes k:80                       0.87     0.77\n",
       "Naive Bayes k:90                       0.87     0.77\n",
       "Bagging Classifier k:20                0.88     0.76\n",
       "Naive Bayes fit_prior:False k:10       0.83     0.76\n",
       "Naive Bayes k:70                       0.86     0.76\n",
       "Naive Bayes k:60                       0.86     0.76\n",
       "Naive Bayes k:50                       0.85     0.76\n",
       "Decision Tree k:20                     0.88     0.76\n",
       "Naive Bayes k:40                       0.85     0.75\n",
       "Naive Bayes k:30                       0.84     0.74\n",
       "Naive Bayes k:20                       0.84     0.73\n",
       "Naive Bayes k:10                       0.82     0.72\n",
       "Decision Tree k:10                     0.86     0.71\n",
       "Bagging Classifier k:10                0.86     0.71\n",
       "baseline                               0.79     0.50"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll evaluate the models.\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Evaluating {name} model, ', end = '')\n",
    "    \n",
    "    start = time()\n",
    "    eval_df = append_model_results(\n",
    "        name,\n",
    "        evaluate(train.cleared, model.make_predictions(train_scaled), True),\n",
    "        eval_df\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds')\n",
    "    \n",
    "eval_df.sort_values(by = 'roc_auc', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f68b5d25-14e3-4b71-8553-20b976e53cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Bagging Classifier k:100 model, 0.889296293258667 seconds\n",
      "Evaluating Bagging Classifier k:90 model, 0.7825591564178467 seconds\n",
      "Evaluating Bagging Classifier k:80 model, 0.8530402183532715 seconds\n",
      "Evaluating Bagging Classifier k:70 model, 0.7350919246673584 seconds\n",
      "Evaluating Bagging Classifier k:60 model, 0.524116039276123 seconds\n",
      "Evaluating Decision Tree k:100 model, 0.08946681022644043 seconds\n",
      "Evaluating Bagging Classifier k:50 model, 0.5005710124969482 seconds\n",
      "Evaluating Decision Tree k:90 model, 0.09373998641967773 seconds\n",
      "Evaluating Decision Tree k:80 model, 0.07422590255737305 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:60 model, 0.1244499683380127 seconds\n",
      "Evaluating Naive Bayes fit_prior:False k:90 model, 0.1652529239654541 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:60</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes fit_prior:False k:90</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:100</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:90</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:80</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:70</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:60</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier k:50</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:100</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:90</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree k:80</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  accuracy  roc_auc\n",
       "Naive Bayes fit_prior:False k:60      0.85     0.82\n",
       "Naive Bayes fit_prior:False k:90      0.83     0.82\n",
       "Bagging Classifier k:100              0.88     0.81\n",
       "Bagging Classifier k:90               0.89     0.81\n",
       "Bagging Classifier k:80               0.89     0.81\n",
       "Bagging Classifier k:70               0.89     0.81\n",
       "Bagging Classifier k:60               0.89     0.81\n",
       "Bagging Classifier k:50               0.89     0.80\n",
       "Decision Tree k:100                   0.89     0.79\n",
       "Decision Tree k:90                    0.89     0.79\n",
       "Decision Tree k:80                    0.89     0.79"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll evaluate the models on validate.\n",
    "\n",
    "algorithms = [\n",
    "    'Bagging Classifier k:100',\n",
    "    'Bagging Classifier k:90',\n",
    "    'Bagging Classifier k:80',\n",
    "    'Bagging Classifier k:70',\n",
    "    'Bagging Classifier k:60',\n",
    "    'Decision Tree k:100',\n",
    "    'Bagging Classifier k:50',\n",
    "    'Decision Tree k:90',\n",
    "    'Decision Tree k:80',\n",
    "    'Naive Bayes fit_prior:False k:60',\n",
    "    'Naive Bayes fit_prior:False k:90'\n",
    "]\n",
    "\n",
    "eval_df = None\n",
    "\n",
    "for model in algorithms:\n",
    "    print(f'Evaluating {model} model, ', end = '')\n",
    "    \n",
    "    start = time()\n",
    "    eval_df = append_model_results(\n",
    "        model,\n",
    "        evaluate(validate.cleared, models[model].make_predictions(validate_scaled), True),\n",
    "        eval_df\n",
    "    )\n",
    "    \n",
    "    end = time()\n",
    "    print(f'{end - start} seconds')\n",
    "    \n",
    "eval_df.sort_values(by = 'roc_auc', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f62b18-aab8-455b-8461-21b3ecae26c8",
   "metadata": {},
   "source": [
    "## Evaluate Best Model on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae9a62ff-6276-4d72-a4e7-eeeb6289778f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  roc_auc\n",
       "Naive Bayes      0.89     0.81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_model_results(\n",
    "    'Naive Bayes',\n",
    "    evaluate(test.cleared, models['Naive Bayes'].make_predictions(test), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb0803-dae5-412d-a218-65d22fce4d1d",
   "metadata": {},
   "source": [
    "The Naive Bayes model is 89% accurate on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
